#!/bin/bash

# =============================================================================
# Advanced Reconnaissance Script for Bug Hunting
# =============================================================================
# This script performs comprehensive reconnaissance by:
# 1. Discovering subdomains using multiple tools
# 2. Extracting URLs using Katana and Gau
# 3. Filtering URLs to focus on JavaScript files and sensitive documents
# =============================================================================

set -e  # Exit on any error

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Script configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
CURRENT_DIR="$(pwd)"
OUTPUT_DIR="${CURRENT_DIR}/recon_output"
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
LOG_FILE="${OUTPUT_DIR}/recon_${TIMESTAMP}.log"
SCRIPT_START_TIME=$(date +%s)
CONFIG_FILE="/usr/local/bin/rezon.conf"

# Default configuration values
MAX_PARALLEL_JOBS=4
GENERATE_STATISTICS=true
VERBOSE_LOGGING=false
WEBHOOK_ENABLED=false
WEBHOOK_URL=""

# Required tools
REQUIRED_TOOLS=("subfinder" "findomain" "katana" "gau" "httpx-toolkit" "nmap" "gf" "jshunter" "nuclei")
REQUIRED_TOOL_PATHS=("/usr/local/bin/uro")

# Unwanted URL extensions to filter out
UNWANTED_EXTENSIONS="png|css|gif|jpeg|woff|svg|jpg|woff2|gif|svg"

# Sensitive file extensions to look for
SENSITIVE_EXTENSIONS="xls|xml|xlsx|json|pdf|sql|doc|docx|pptx|txt|zip|tar\.gz|tgz|bak|7z|rar|log|cache|secret|db|backup|yml|gz|config|csv|yaml|md|md5|exe|dll|bin|ini|bat|sh|tar|deb|git|env|rpm|iso|img|apk|msi|dmg|tmp|crt|pem|key|pub|asc"

# Comprehensive port list for httpx-toolkit
# Web / portals / developer web apps
WEB_PORTS="80,443,8080,8000,3000,4200,5000,8443,9000,10000,2082,2083,2086,2087,8888"
# Developer tools / repos / CI
DEV_PORTS="22,9418,8081,8082,8090,50000"
# Databases & data stores
DB_PORTS="27017,27018,27019,5984,9200,9300,5432,3306,6379,11211"
# Message queues / admin consoles
ADMIN_PORTS="15672,8161,5601"
# Directory, LDAP, AD & Windows services
DIR_PORTS="389,636,3268,3269,135,139,445,3389"
# VPNs and remote access
VPN_PORTS="1194,443,943,9443,500,4500,1701,1723,51820"
# Container / orchestration
CONTAINER_PORTS="6443,10250,10255,2375,2376"
# Monitoring / misc
MONITOR_PORTS="161,162,5060,5061,25,587,465"

# Combined port list for httpx-toolkit
HTTPX_PORTS="$WEB_PORTS,$DEV_PORTS,$DB_PORTS,$ADMIN_PORTS,$DIR_PORTS,$VPN_PORTS,$CONTAINER_PORTS,$MONITOR_PORTS"

# Comprehensive port list for nmap scanning
# Web services, databases, and common services
NMAP_PORTS="21,22,23,25,53,80,110,111,135,139,143,443,993,995,1723,3306,3389,5432,5900,8080,8443,8888,9000,10000,27017,50070,6379,9200,11211,15672,8161,5601,6443,2375,2376,10250,10255,1194,500,4500,1701,51820,389,636,3268,3269,161,162,5060,5061,587,465,9418,8081,8082,8090,50000,2082,2083,2086,2087,3000,4200,5000,5984,9300,27018,27019,11211,943,9443,1723,6443,10250,10255,2375,2376,161,162,5060,5061,25,587,465,79,5101,199,544,646,5190,4899,8009"

# =============================================================================
# Utility Functions
# =============================================================================

# Load configuration file
load_config() {
    if [[ -f "$CONFIG_FILE" ]]; then
        log "Loading configuration from: $CONFIG_FILE"
        source "$CONFIG_FILE"
        log_success "Configuration loaded successfully"
    else
        log_warning "Configuration file not found: $CONFIG_FILE (using defaults)"
    fi
}

# Validate domain format
validate_domain() {
    local domain="$1"
    # Basic domain validation regex
    if [[ "$domain" =~ ^[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?(\.[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?)*$ ]]; then
        return 0
    else
        return 1
    fi
}

# Validate file exists and is readable
validate_file() {
    local file="$1"
    if [[ -f "$file" && -r "$file" ]]; then
        return 0
    else
        return 1
    fi
}

# Clean and validate domain list
clean_domain_list() {
    local input_file="$1"
    local output_file="$2"
    
    log "Cleaning and validating domain list..."
    local valid_count=0
    local invalid_count=0
    
    > "$output_file"
    
    while IFS= read -r domain || [ -n "$domain" ]; do
        # Skip empty lines and comments
        if [[ -z "$domain" || "$domain" =~ ^[[:space:]]*# ]]; then
            continue
        fi
        
        # Remove leading/trailing whitespace
        domain=$(echo "$domain" | xargs)
        
        if [[ -n "$domain" ]]; then
            if validate_domain "$domain"; then
                echo "$domain" >> "$output_file"
                ((valid_count++))
            else
                log_warning "Invalid domain format: $domain"
                ((invalid_count++))
            fi
        fi
    done < "$input_file"
    
    log_success "Domain validation completed: $valid_count valid, $invalid_count invalid"
    
    if [ $valid_count -eq 0 ]; then
        return 1  # No valid domains found
    else
        return 0  # Success - valid domains found
    fi
}

log() {
    # Ensure log directory exists
    mkdir -p "$(dirname "$LOG_FILE")"
    echo -e "${BLUE}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} $1" | tee -a "$LOG_FILE"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1" | tee -a "$LOG_FILE"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1" | tee -a "$LOG_FILE"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1" | tee -a "$LOG_FILE"
}

# Progress bar function
show_progress() {
    local current=$1
    local total=$2
    local description="$3"
    local width=50
    local percentage=$((current * 100 / total))
    local filled=$((current * width / total))
    local empty=$((width - filled))
    
    printf "\r${BLUE}[PROGRESS]${NC} %s [%s%s] %d%% (%d/%d)" \
        "$description" \
        "$(printf "%*s" $filled | tr ' ' '=')" \
        "$(printf "%*s" $empty | tr ' ' '-')" \
        "$percentage" \
        "$current" \
        "$total"
    
    if [[ $current -eq $total ]]; then
        echo ""
    fi
}

# Spinner for long operations
show_spinner() {
    local pid=$1
    local description="$2"
    local delay=0.1
    local spinstr='|/-\'
    
    while [[ $(ps -p $pid -o pid=) ]]; do
        local temp=${spinstr#?}
        printf "\r${BLUE}[WORKING]${NC} %s %c" "$description" "$spinstr"
        local spinstr=$temp${spinstr%"$temp"}
        sleep $delay
    done
    printf "\r${GREEN}[DONE]${NC} %s completed\n" "$description"
}

# Check if required tools are installed
check_tools() {
    log "Checking for required tools..."
    local missing_tools=()
    
    # Check tools in PATH
    for tool in "${REQUIRED_TOOLS[@]}"; do
        if ! command -v "$tool" &> /dev/null; then
            missing_tools+=("$tool")
        else
            log_success "$tool is installed"
        fi
    done
    
    # Check tools with specific paths
    for tool_path in "${REQUIRED_TOOL_PATHS[@]}"; do
        if [ ! -f "$tool_path" ]; then
            missing_tools+=("$tool_path")
        else
            log_success "$tool_path is installed"
        fi
    done
    
    if [ ${#missing_tools[@]} -ne 0 ]; then
        log_error "Missing required tools: ${missing_tools[*]}"
        log "Please install the missing tools before running this script."
        log "Run './install_tools.sh' to install all required tools."
        send_error_webhook "Missing required tools" "Tools missing: ${missing_tools[*]}"
        exit 1
    fi
}

# Create output directories
setup_directories() {
    log "Setting up output directories..."
    mkdir -p "$OUTPUT_DIR"/{subdomains,alive_domains,urls,js_urls,sensitive_urls,logs}
    log_success "Output directories created"
}

# Clean up temporary files
cleanup() {
    log "Cleaning up temporary files..."
    rm -f "$OUTPUT_DIR"/temp_*.txt
    log_success "Cleanup completed"
}

# =============================================================================
# Subdomain Discovery Functions
# =============================================================================

# Run subfinder
run_subfinder() {
    local domain="$1"
    local output_file="$2"
    log "Running subfinder for $domain..."
    
    if subfinder -d "$domain" -silent -o "$output_file" 2>>"$LOG_FILE"; then
        log_success "Subfinder completed for $domain"
    else
        log_warning "Subfinder failed for $domain"
    fi
}

# Run subfinder with domain list file
run_subfinder_file() {
    local domains_file="$1"
    local output_file="$OUTPUT_DIR/subdomains/all_subfinder_subdomains.txt"
    log "Running subfinder for all domains from file..."
    
    if subfinder -dL "$domains_file" -silent -o "$output_file" 2>>"$LOG_FILE"; then
        local count=$(wc -l < "$output_file" 2>/dev/null || echo 0)
        log_success "Subfinder completed for all domains - found $count subdomains"
    else
        log_warning "Subfinder failed for domain file"
    fi
}

# Run findomain
run_findomain() {
    local domain="$1"
    local output_file="$2"
    log "Running findomain for $domain..."
    
    if findomain -t "$domain" -q 2>>"$LOG_FILE" | tee -a "$output_file"; then
        log_success "Findomain completed for $domain"
    else
        log_warning "Findomain failed for $domain"
    fi
}

# Run findomain with domain list file
run_findomain_file() {
    local domains_file="$1"
    local output_file="$OUTPUT_DIR/subdomains/all_findomain_subdomains.txt"
    log "Running findomain for all domains from file..."
    
    if findomain -f "$domains_file" -q -u "$output_file" 2>>"$LOG_FILE"; then
        local count=$(wc -l < "$output_file" 2>/dev/null || echo 0)
        log_success "Findomain completed for all domains - found $count subdomains"
    else
        log_warning "Findomain failed for domain file"
    fi
}


# Run assetfinder
run_assetfinder() {
    local domain="$1"
    local output_file="$2"
    log "Running assetfinder for $domain..."
    
    if assetfinder -subs-only "$domain" 2>>"$LOG_FILE" | tee -a "$output_file"; then
        log_success "Assetfinder completed for $domain"
    else
        log_warning "Assetfinder failed for $domain"
    fi
}

# Run assetfinder with domain list file using stdin
run_assetfinder_file() {
    local domains_file="$1"
    local output_file="$OUTPUT_DIR/subdomains/all_assetfinder_subdomains.txt"
    log "Running assetfinder for all domains from file using stdin..."
    
    if cat "$domains_file" | assetfinder -subs-only 2>>"$LOG_FILE" | tee -a "$output_file"; then
        local count=$(wc -l < "$output_file" 2>/dev/null || echo 0)
        log_success "Assetfinder completed for all domains - found $count subdomains"
    else
        log_warning "Assetfinder failed for domain file"
    fi
}

# Run subdominator (if available)
run_subdominator() {
    local domain="$1"
    local output_file="$2"
    
    if command -v subdominator &> /dev/null; then
        log "Running subdominator for $domain..."
        if subdominator -d "$domain" -o "$OUTPUT_DIR/temp_subdominator_$domain.txt" 2>>"$LOG_FILE"; then
            if [ -f "$OUTPUT_DIR/temp_subdominator_$domain.txt" ]; then
                cat "$OUTPUT_DIR/temp_subdominator_$domain.txt" >> "$output_file"
                rm -f "$OUTPUT_DIR/temp_subdominator_$domain.txt"
            fi
            log_success "Subdominator completed for $domain"
        else
            log_warning "Subdominator failed for $domain"
        fi
    else
        log_warning "Subdominator not found, skipping..."
    fi
}

# Run subdominator with domain list file
run_subdominator_file() {
    local domains_file="$1"
    local output_file="$OUTPUT_DIR/subdomains/all_subdominator_subdomains.txt"
    
    if command -v subdominator &> /dev/null; then
        log "Running subdominator for all domains from file..."
        
        if subdominator -dL "$domains_file" -o "$output_file" 2>>"$LOG_FILE"; then
            local count=$(wc -l < "$output_file" 2>/dev/null || echo 0)
            log_success "Subdominator completed for all domains - found $count subdomains"
        else
            log_warning "Subdominator failed for domain file"
        fi
    else
        log_warning "Subdominator not found, skipping"
    fi
}

# Discover subdomains for a single domain
discover_subdomains() {
    local domain="$1"
    local output_file="$OUTPUT_DIR/subdomains/${domain}_subdomains.txt"
    
    log "Starting subdomain discovery for $domain..."
    
    # Initialize output file
    > "$output_file"
    
    # Run all subdomain discovery tools
    run_subfinder "$domain" "$output_file"
    run_findomain "$domain" "$output_file"
    run_assetfinder "$domain" "$output_file"
    run_subdominator "$domain" "$output_file"
    
    # Remove duplicates and sort
    if [ -s "$output_file" ]; then
        sort -u "$output_file" -o "$output_file"
        local count=$(wc -l < "$output_file")
        log_success "Found $count unique subdomains for $domain"
    else
        log_warning "No subdomains found for $domain"
    fi
}

# =============================================================================
# URL Extraction Functions
# =============================================================================

# Run katana for URL extraction
run_katana() {
    local output_file="$OUTPUT_DIR/urls/all_katana_urls.txt"
    local temp_urls_file="$OUTPUT_DIR/temp_katana_urls.txt"
    log "Running katana against all discovered subdomains..."
    
    # Use all combined subdomains
    if [ -f "$OUTPUT_DIR/all_subdomains.txt" ] && [ -s "$OUTPUT_DIR/all_subdomains.txt" ]; then
        # Create a temporary file with proper URLs (http:// and https://)
        > "$temp_urls_file"
        while IFS= read -r subdomain || [ -n "$subdomain" ]; do
            # Remove leading/trailing whitespace
            subdomain=$(echo "$subdomain" | xargs)
            if [ -n "$subdomain" ]; then
                echo "http://$subdomain" >> "$temp_urls_file"
                echo "https://$subdomain" >> "$temp_urls_file"
            fi
        done < "$OUTPUT_DIR/all_subdomains.txt"
        
        # Run katana with the properly formatted URL list
        if katana -list "$temp_urls_file" -silent -depth 3 -jc -o "$output_file" 2>>"$LOG_FILE"; then
            local katana_count=$(wc -l < "$output_file" 2>/dev/null || echo 0)
            log_success "Katana completed for all subdomains - found $katana_count URLs"
        else
            log_warning "Katana failed"
        fi
        
        # Clean up temporary file
        rm -f "$temp_urls_file"
    else
        log_warning "No subdomains found to crawl with katana"
    fi
}

# Run gau for URL extraction
run_gau() {
    local output_file="$OUTPUT_DIR/urls/all_gau_urls.txt"
    log "Running gau against all discovered subdomains..."
    
    # Use all combined subdomains
    if [ -f "$OUTPUT_DIR/all_subdomains.txt" ] && [ -s "$OUTPUT_DIR/all_subdomains.txt" ]; then
        # Run gau using stdin approach: cat file | gau
        if cat "$OUTPUT_DIR/all_subdomains.txt" | gau --subs --threads 50 --blacklist "$UNWANTED_EXTENSIONS" 2>>"$LOG_FILE" | tee -a "$output_file"; then
            local count=$(wc -l < "$output_file" 2>/dev/null || echo 0)
            log_success "Gau completed for all subdomains - found $count URLs"
        else
            log_warning "Gau failed"
        fi
    else
        log_warning "No subdomains found to fetch URLs with gau"
    fi
}

run_httpx() {
    local output_file="$OUTPUT_DIR/alive_domains/all_alive_domains.txt"
    log "Running httpx-toolkit against all discovered subdomains with comprehensive port scanning..."
    
    # Use all combined subdomains
    if [ -f "$OUTPUT_DIR/all_subdomains.txt" ] && [ -s "$OUTPUT_DIR/all_subdomains.txt" ]; then
        local subdomain_count=$(wc -l < "$OUTPUT_DIR/all_subdomains.txt")
        log "Scanning $subdomain_count subdomains across comprehensive port ranges..."
        echo ""
        
        # Run httpx-toolkit with comprehensive options
        # -sc: status code, -server: web server, -td: tech detect, -p: ports
        # Remove -silent to show live results
        if httpx-toolkit -l "$OUTPUT_DIR/all_subdomains.txt" -sc -server -td -p "$HTTPX_PORTS" -threads 50 -timeout 10 -follow-redirects 2>>"$LOG_FILE" | tee -a "$output_file"; then
            echo ""
            local alive_count=$(wc -l < "$output_file" 2>/dev/null || echo 0)
            log_success "httpx-toolkit completed - found $alive_count alive domains/services"
        else
            log_warning "httpx-toolkit failed"
        fi
    else
        log_warning "No subdomains found to scan with httpx-toolkit"
    fi
}

# Extract clean domains from httpx output and run nmap
run_nmap() {
    local alive_domains_file="$OUTPUT_DIR/alive_domains/all_alive_domains.txt"
    local clean_domains_file="$OUTPUT_DIR/alive_domains/alive-domains.txt"
    local nmap_output="$OUTPUT_DIR/nmap_results.txt"
    
    log "Extracting clean domains from httpx output for nmap scanning..."
    
    # Check if httpx output exists
    if [ -f "$alive_domains_file" ] && [ -s "$alive_domains_file" ]; then
        # Extract clean domains using the provided awk/sed commands
        awk '{print $1}' "$alive_domains_file" \
            | sed -E 's#^[a-zA-Z]+://##; s#^[^@]*@##; s#/.*##; s/:.*$//' \
            | sort -u > "$clean_domains_file"
        
        local domain_count=$(wc -l < "$clean_domains_file" 2>/dev/null || echo 0)
        
        if [ "$domain_count" -gt 0 ]; then
            log "Running nmap against $domain_count alive domains with comprehensive port scanning..."
            echo ""
            
            # Run nmap with comprehensive options
            # -sVC: service version detection, -sU: UDP scan
            # -p: comprehensive port list, -iL: input file, -oN: output file
            if nmap -sVC -sU -p "$NMAP_PORTS" -iL "$clean_domains_file" -oN "$nmap_output" --max-retries 2 --max-rtt-timeout 1000ms 2>>"$LOG_FILE"; then
                echo ""
                log_success "Nmap completed - results saved to $nmap_output"
                
                # Show summary of open ports
                local open_ports=$(grep -c "open" "$nmap_output" 2>/dev/null || echo 0)
                log "Found $open_ports open ports across all scanned domains"
            else
                log_warning "Nmap failed"
            fi
        else
            log_warning "No valid domains extracted from httpx output"
        fi
    else
        log_warning "No httpx output found - skipping nmap scan"
    fi
}

# Extract URLs for a single domain
extract_urls() {
    local combined_output="$OUTPUT_DIR/urls/all_combined_urls.txt"
    
    log "Starting URL extraction from all discovered subdomains..."
    
    # Initialize output files
    > "$OUTPUT_DIR/urls/all_katana_urls.txt"
    > "$OUTPUT_DIR/urls/all_gau_urls.txt"
    > "$combined_output"
    
    # Run URL extraction tools against all subdomains
    run_katana
    run_gau
    
    # Combine and deduplicate URLs
    if [ -s "$OUTPUT_DIR/urls/all_katana_urls.txt" ] || [ -s "$OUTPUT_DIR/urls/all_gau_urls.txt" ]; then
        cat "$OUTPUT_DIR/urls/all_katana_urls.txt" "$OUTPUT_DIR/urls/all_gau_urls.txt" | sort -u > "$combined_output"
        local count=$(wc -l < "$combined_output")
        log_success "Found $count unique URLs from all subdomains"
    else
        log_warning "No URLs found from subdomains"
    fi
}

# =============================================================================
# URL Filtering Functions
# =============================================================================

# Filter JavaScript URLs
filter_js_urls() {
    local input_file="$OUTPUT_DIR/urls/all_combined_urls.txt"
    local output_file="$OUTPUT_DIR/js_urls/all_js_urls.txt"
    
    log "Filtering JavaScript URLs from all discovered URLs..."
    
    if [ -s "$input_file" ]; then
        grep '\.js$' "$input_file" > "$output_file"
        local count=$(wc -l < "$output_file")
        log_success "Found $count JavaScript URLs from all subdomains"
    else
        log_warning "No URLs to filter"
        > "$output_file"
    fi
}

# Filter sensitive URLs
filter_sensitive_urls() {
    local input_file="$OUTPUT_DIR/urls/all_combined_urls.txt"
    local output_file="$OUTPUT_DIR/sensitive_urls/all_sensitive_urls.txt"
    
    log "Filtering sensitive URLs from all discovered URLs..."
    
    if [ -s "$input_file" ]; then
        # First filter out unwanted extensions, then look for sensitive ones
        grep -vE "\.($UNWANTED_EXTENSIONS)$" "$input_file" | \
        /usr/local/bin/uro | \
        grep -E "\.($SENSITIVE_EXTENSIONS)$" > "$output_file"
        
        local count=$(wc -l < "$output_file")
        log_success "Found $count sensitive URLs from all subdomains"
    else
        log_warning "No URLs to filter"
        > "$output_file"
    fi
}

# Filter XSS URLs using gf
filter_xss_urls() {
    local input_file="$OUTPUT_DIR/urls/all_combined_urls.txt"
    local output_file="$OUTPUT_DIR/xss_urls.txt"
    
    log "Filtering XSS URLs using gf patterns..."
    
    if [ -s "$input_file" ]; then
        if cat "$input_file" | gf xss | /usr/local/bin/uro | tee "$output_file"; then
            local count=$(wc -l < "$output_file" 2>/dev/null || echo 0)
            log_success "Found $count potential XSS URLs"
        else
            log_warning "XSS URL filtering failed"
            > "$output_file"
        fi
    else
        log_warning "No URLs to filter for XSS"
        > "$output_file"
    fi
}

# Filter SQL injection URLs using gf
filter_sqli_urls() {
    local input_file="$OUTPUT_DIR/urls/all_combined_urls.txt"
    local output_file="$OUTPUT_DIR/sqli_urls.txt"
    
    log "Filtering SQL injection URLs using gf patterns..."
    
    if [ -s "$input_file" ]; then
        if cat "$input_file" | gf sqli | tee "$output_file"; then
            local count=$(wc -l < "$output_file" 2>/dev/null || echo 0)
            log_success "Found $count potential SQL injection URLs"
        else
            log_warning "SQL injection URL filtering failed"
            > "$output_file"
        fi
    else
        log_warning "No URLs to filter for SQL injection"
        > "$output_file"
    fi
}

# Run jshunter on JavaScript URLs
run_jshunter() {
    local input_file="$OUTPUT_DIR/js_urls/all_js_urls.txt"
    local output_file="$OUTPUT_DIR/jsurls.txt"
    
    log "Running jshunter on JavaScript URLs..."
    
    if [ -s "$input_file" ]; then
        local js_count=$(wc -l < "$input_file")
        log "Analyzing $js_count JavaScript URLs with jshunter..."
        
        # Load webhook configuration if available
        local webhook_args=""
        if [ "$WEBHOOK_ENABLED" = true ] && [ -n "$WEBHOOK_URL" ]; then
            webhook_args="--webhook $WEBHOOK_URL"
            log "Using webhook notification: $WEBHOOK_URL"
        fi
        
        # Run jshunter with webhook if configured
        if jshunter -f "$input_file" -o "$output_file" $webhook_args 2>>"$LOG_FILE"; then
            local hunter_count=$(wc -l < "$output_file" 2>/dev/null || echo 0)
            log_success "jshunter completed - found $hunter_count interesting JavaScript endpoints"
        else
            log_warning "jshunter failed"
            > "$output_file"
        fi
    else
        log_warning "No JavaScript URLs to analyze with jshunter"
        > "$output_file"
    fi
}

# Run nuclei on JavaScript URLs with tokens templates
run_nuclei_js() {
    local input_file="$OUTPUT_DIR/jsurls.txt"
    local output_file="$OUTPUT_DIR/nuclei_js_results.txt"
    local tokens_templates="/root/.local/nuclei-templates/http/exposures/tokens/"
    
    log "Running nuclei on JavaScript URLs with tokens templates..."
    
    if [ -s "$input_file" ]; then
        local js_count=$(wc -l < "$input_file")
        log "Scanning $js_count JavaScript URLs with nuclei tokens templates..."
        
        # Check if templates directory exists
        if [ -d "$tokens_templates" ]; then
            # Run nuclei with tokens templates
            if nuclei -list "$input_file" -t "$tokens_templates" -rl 5 -o "$output_file" 2>>"$LOG_FILE"; then
                local nuclei_count=$(wc -l < "$output_file" 2>/dev/null || echo 0)
                log_success "Nuclei JS scanning completed - found $nuclei_count potential issues"
            else
                log_warning "Nuclei JS scanning failed"
                > "$output_file"
            fi
        else
            log_warning "Tokens templates directory not found: $tokens_templates"
            log "Skipping nuclei JS scanning"
            > "$output_file"
        fi
    else
        log_warning "No JavaScript URLs to scan with nuclei"
        > "$output_file"
    fi
}

# Run nuclei on alive domains with rezon templates
run_nuclei_alive() {
    local alive_domains_file="$OUTPUT_DIR/alive_domains/all_alive_domains.txt"
    local clean_domains_file="$OUTPUT_DIR/alivedomains.txt"
    local output_file="$OUTPUT_DIR/nuclei_alive_results.txt"
    local rezon_templates="$HOME/.local/nuclei-templates/http/rezon-templates"
    
    log "Running nuclei on alive domains with rezon templates..."
    
    # Check if httpx output exists
    if [ -f "$alive_domains_file" ] && [ -s "$alive_domains_file" ]; then
        # Extract clean URLs using the provided command
        cat "$alive_domains_file" | awk '{print $1}' | tee "$clean_domains_file"
        
        local domain_count=$(wc -l < "$clean_domains_file" 2>/dev/null || echo 0)
        
        if [ "$domain_count" -gt 0 ]; then
            log "Scanning $domain_count alive domains with nuclei rezon templates..."
            
            # Check if templates directory exists
            if [ -d "$rezon_templates" ]; then
                # Run nuclei with rezon templates
                if nuclei -list "$clean_domains_file" -t "$rezon_templates" -rl 5 -o "$output_file" 2>>"$LOG_FILE"; then
                    local nuclei_count=$(wc -l < "$output_file" 2>/dev/null || echo 0)
                    log_success "Nuclei alive scanning completed - found $nuclei_count potential issues"
                else
                    log_warning "Nuclei alive scanning failed"
                    > "$output_file"
                fi
            else
                log_warning "Rezon templates directory not found: $rezon_templates"
                log "Skipping nuclei alive scanning"
                > "$output_file"
            fi
        else
            log_warning "No valid domains extracted for nuclei scanning"
            > "$output_file"
        fi
    else
        log_warning "No httpx output found - skipping nuclei alive scanning"
        > "$output_file"
    fi
}

# Run nuclei tech detection on alive domains
run_nuclei_tech() {
    local alive_domains_file="$OUTPUT_DIR/alive_domains/all_alive_domains.txt"
    local clean_domains_file="$OUTPUT_DIR/alivedomains.txt"
    local output_file="$OUTPUT_DIR/alivedomainstech.txt"
    
    log "Running nuclei tech detection on alive domains..."
    
    # Check if httpx output exists
    if [ -f "$alive_domains_file" ] && [ -s "$alive_domains_file" ]; then
        # Extract clean URLs using the provided command
        cat "$alive_domains_file" | awk '{print $1}' | tee "$clean_domains_file"
        
        local domain_count=$(wc -l < "$clean_domains_file" 2>/dev/null || echo 0)
        
        if [ "$domain_count" -gt 0 ]; then
            log "Running tech detection on $domain_count alive domains with nuclei..."
            
            # Run nuclei with tech tags
            if nuclei -list "$clean_domains_file" -tags tech -o "$output_file" 2>>"$LOG_FILE"; then
                local tech_count=$(wc -l < "$output_file" 2>/dev/null || echo 0)
                log_success "Nuclei tech detection completed - found $tech_count technology signatures"
            else
                log_warning "Nuclei tech detection failed"
                > "$output_file"
            fi
        else
            log_warning "No valid domains extracted for tech detection"
            > "$output_file"
        fi
    else
        log_warning "No httpx output found - skipping nuclei tech detection"
        > "$output_file"
    fi
}

# Create combined output files
create_combined_outputs() {
    log "Creating combined output files..."
    
    # all_subdomains.txt is already created in process_all_subdomains()
    
    # Copy combined URLs (already created by extract_urls)
    if [ -f "$OUTPUT_DIR/urls/all_combined_urls.txt" ]; then
        cp "$OUTPUT_DIR/urls/all_combined_urls.txt" "$OUTPUT_DIR/all_urls.txt"
    fi
    
    # Copy combined JavaScript URLs (already created by filter_js_urls)
    if [ -f "$OUTPUT_DIR/js_urls/all_js_urls.txt" ]; then
        cp "$OUTPUT_DIR/js_urls/all_js_urls.txt" "$OUTPUT_DIR/all_js_urls.txt"
    fi
    
    # Copy combined sensitive URLs (already created by filter_sensitive_urls)
    if [ -f "$OUTPUT_DIR/sensitive_urls/all_sensitive_urls.txt" ]; then
        cp "$OUTPUT_DIR/sensitive_urls/all_sensitive_urls.txt" "$OUTPUT_DIR/sensitiveurls.txt"
    fi
    
    # Create filtered URLs (excluding unwanted extensions)
    if [ -f "$OUTPUT_DIR/all_urls.txt" ]; then
        grep -vE "\.($UNWANTED_EXTENSIONS)$" "$OUTPUT_DIR/all_urls.txt" > "$OUTPUT_DIR/filtered_urls.txt"
    fi
    
    log_success "Combined output files created"
}

# Generate detailed statistics
generate_statistics() {
    log "Generating detailed statistics..."
    
    local stats_file="$OUTPUT_DIR/statistics.txt"
    local start_time=$(date -d "@$SCRIPT_START_TIME" '+%Y-%m-%d %H:%M:%S')
    local end_time=$(date '+%Y-%m-%d %H:%M:%S')
    local duration=$(( $(date +%s) - SCRIPT_START_TIME ))
    
    {
        echo "============================================================================="
        echo "                    RECONNAISSANCE STATISTICS REPORT"
        echo "============================================================================="
        echo ""
        echo "Execution Details:"
        echo "  Start Time: $start_time"
        echo "  End Time:   $end_time"
        echo "  Duration:   ${duration}s ($(($duration / 60))m $(($duration % 60))s)"
        echo ""
        echo "Input Summary:"
        echo "  Domains Processed: $(ls "$OUTPUT_DIR"/subdomains/*_subdomains.txt 2>/dev/null | wc -l)"
        echo "  Alive Domains:     $(ls "$OUTPUT_DIR"/alive_domains/*_alive_domains.txt 2>/dev/null | wc -l)"
        echo ""
        echo "Discovery Results:"
        echo "  Total Subdomains: $(wc -l < "$OUTPUT_DIR/all_subdomains.txt" 2>/dev/null || echo 0)"
        echo "  Total URLs:       $(wc -l < "$OUTPUT_DIR/all_urls.txt" 2>/dev/null || echo 0)"
        echo "  JavaScript URLs:  $(wc -l < "$OUTPUT_DIR/all_js_urls.txt" 2>/dev/null || echo 0)"
        echo "  Sensitive URLs:   $(wc -l < "$OUTPUT_DIR/sensitiveurls.txt" 2>/dev/null || echo 0)"
        echo "  XSS URLs:         $(wc -l < "$OUTPUT_DIR/xss_urls.txt" 2>/dev/null || echo 0)"
        echo "  SQLi URLs:        $(wc -l < "$OUTPUT_DIR/sqli_urls.txt" 2>/dev/null || echo 0)"
        echo "  JS Hunter URLs:   $(wc -l < "$OUTPUT_DIR/jsurls.txt" 2>/dev/null || echo 0)"
        echo "  Nuclei JS Issues: $(wc -l < "$OUTPUT_DIR/nuclei_js_results.txt" 2>/dev/null || echo 0)"
        echo "  Nuclei Alive Issues: $(wc -l < "$OUTPUT_DIR/nuclei_alive_results.txt" 2>/dev/null || echo 0)"
        echo "  Tech Signatures: $(wc -l < "$OUTPUT_DIR/alivedomainstech.txt" 2>/dev/null || echo 0)"
        echo "  Filtered URLs:    $(wc -l < "$OUTPUT_DIR/filtered_urls.txt" 2>/dev/null || echo 0)"
        echo "  Alive Domains:    $(wc -l < "$OUTPUT_DIR/alive_domains/alive-domains.txt" 2>/dev/null || echo 0)"
        echo "  Open Ports:       $(grep -c "open" "$OUTPUT_DIR/nmap_results.txt" 2>/dev/null || echo 0)"
        echo ""
        echo "Top Subdomains by Count:"
        if [[ -f "$OUTPUT_DIR/all_subdomains.txt" ]]; then
            cut -d'.' -f1 "$OUTPUT_DIR/all_subdomains.txt" | sort | uniq -c | sort -nr | head -10 | while read count subdomain; do
                echo "  $count: $subdomain"
            done
        fi
        echo ""
        echo "URL Extension Analysis:"
        if [[ -f "$OUTPUT_DIR/all_urls.txt" ]]; then
            grep -oE '\.[a-zA-Z0-9]+$' "$OUTPUT_DIR/all_urls.txt" | sort | uniq -c | sort -nr | head -10 | while read count ext; do
                echo "  $count: $ext"
            done
        fi
        echo ""
        echo "Sensitive File Types Found:"
        if [[ -f "$OUTPUT_DIR/sensitiveurls.txt" ]]; then
            grep -oE '\.[a-zA-Z0-9]+$' "$OUTPUT_DIR/sensitiveurls.txt" | sort | uniq -c | sort -nr | while read count ext; do
                echo "  $count: $ext"
            done
        fi
        echo ""
        echo "Tool Performance:"
        echo "  Subfinder: $(grep -c "Subfinder completed" "$LOG_FILE" 2>/dev/null || echo 0) successful runs"
        echo "  Findomain: $(grep -c "Findomain completed" "$LOG_FILE" 2>/dev/null || echo 0) successful runs"
        echo "  Sublist3r: $(grep -c "Sublist3r completed" "$LOG_FILE" 2>/dev/null || echo 0) successful runs"
        echo "  Assetfinder: $(grep -c "Assetfinder completed" "$LOG_FILE" 2>/dev/null || echo 0) successful runs"
        echo "  httpx-toolkit: $(grep -c "httpx-toolkit completed" "$LOG_FILE" 2>/dev/null || echo 0) successful runs"
        echo "  Nmap: $(grep -c "Nmap completed" "$LOG_FILE" 2>/dev/null || echo 0) successful runs"
        echo "  Katana: $(grep -c "Katana completed" "$LOG_FILE" 2>/dev/null || echo 0) successful runs"
        echo "  Gau: $(grep -c "Gau completed" "$LOG_FILE" 2>/dev/null || echo 0) successful runs"
        echo "  jshunter: $(grep -c "jshunter completed" "$LOG_FILE" 2>/dev/null || echo 0) successful runs"
        echo "  Nuclei Tech: $(grep -c "Nuclei tech detection completed" "$LOG_FILE" 2>/dev/null || echo 0) successful runs"
        echo "  Nuclei JS: $(grep -c "Nuclei JS scanning completed" "$LOG_FILE" 2>/dev/null || echo 0) successful runs"
        echo "  Nuclei Alive: $(grep -c "Nuclei alive scanning completed" "$LOG_FILE" 2>/dev/null || echo 0) successful runs"
        echo ""
        echo "============================================================================="
    } > "$stats_file"
    
    log_success "Statistics report generated: $stats_file"
}

# Send webhook notification (Discord format)
send_webhook_notification() {
    local status="$1"
    local message="$2"
    local details="$3"
    
    if [[ "$WEBHOOK_ENABLED" == "true" && -n "$WEBHOOK_URL" ]]; then
        # Create Discord-formatted message
        local discord_message="**üîç Rezon Reconnaissance Report**\n\n"
        discord_message+="**Status:** $status\n"
        discord_message+="**Message:** $message\n"
        discord_message+="**Details:** $details\n"
        discord_message+="**Timestamp:** $(date '+%Y-%m-%d %H:%M:%S')\n"
        discord_message+="**Output Directory:** \`$OUTPUT_DIR\`"
        
        # Discord webhook payload
        local payload=$(cat <<EOF
{
    "content": "$discord_message",
    "username": "Rezon Bot",
    "avatar_url": "https://cdn.discordapp.com/emojis/1234567890.png"
}
EOF
        )
        
        # Send webhook with better error handling
        local response=$(curl -s -w "%{http_code}" -X POST -H "Content-Type: application/json" -d "$payload" "$WEBHOOK_URL")
        local http_code="${response: -3}"
        
        if [ "$http_code" = "200" ] || [ "$http_code" = "204" ]; then
            log_success "Discord webhook notification sent successfully"
        else
            log_warning "Failed to send webhook notification (HTTP: $http_code)"
            log "Webhook URL: $WEBHOOK_URL"
        fi
    fi
}

# Send detailed results webhook
send_results_webhook() {
    if [[ "$WEBHOOK_ENABLED" == "true" && -n "$WEBHOOK_URL" ]]; then
        local total_subdomains=$(wc -l < "$OUTPUT_DIR/all_subdomains.txt" 2>/dev/null || echo 0)
        local total_urls=$(wc -l < "$OUTPUT_DIR/all_urls.txt" 2>/dev/null || echo 0)
        local total_js=$(wc -l < "$OUTPUT_DIR/all_js_urls.txt" 2>/dev/null || echo 0)
        local total_sensitive=$(wc -l < "$OUTPUT_DIR/sensitiveurls.txt" 2>/dev/null || echo 0)
        local total_xss=$(wc -l < "$OUTPUT_DIR/xss_urls.txt" 2>/dev/null || echo 0)
        local total_sqli=$(wc -l < "$OUTPUT_DIR/sqli_urls.txt" 2>/dev/null || echo 0)
        local total_js_hunter=$(wc -l < "$OUTPUT_DIR/jsurls.txt" 2>/dev/null || echo 0)
        local total_nuclei_js=$(wc -l < "$OUTPUT_DIR/nuclei_js_results.txt" 2>/dev/null || echo 0)
        local total_nuclei_alive=$(wc -l < "$OUTPUT_DIR/nuclei_alive_results.txt" 2>/dev/null || echo 0)
        local total_tech=$(wc -l < "$OUTPUT_DIR/alivedomainstech.txt" 2>/dev/null || echo 0)
        local total_alive=$(wc -l < "$OUTPUT_DIR/alive_domains/alive-domains.txt" 2>/dev/null || echo 0)
        local total_ports=$(grep -c "open" "$OUTPUT_DIR/nmap_results.txt" 2>/dev/null || echo 0)
        
        local discord_message="**üéØ Rezon Reconnaissance Complete!**\n\n"
        discord_message+="**üìä Discovery Results:**\n"
        discord_message+="‚Ä¢ Subdomains: \`$total_subdomains\`\n"
        discord_message+="‚Ä¢ Alive Domains: \`$total_alive\`\n"
        discord_message+="‚Ä¢ Total URLs: \`$total_urls\`\n"
        discord_message+="‚Ä¢ JavaScript URLs: \`$total_js\`\n"
        discord_message+="‚Ä¢ Sensitive URLs: \`$total_sensitive\`\n\n"
        discord_message+="**üîç Security Analysis:**\n"
        discord_message+="‚Ä¢ XSS URLs: \`$total_xss\`\n"
        discord_message+="‚Ä¢ SQLi URLs: \`$total_sqli\`\n"
        discord_message+="‚Ä¢ JS Hunter URLs: \`$total_js_hunter\`\n"
        discord_message+="‚Ä¢ Tech Signatures: \`$total_tech\`\n"
        discord_message+="‚Ä¢ Nuclei JS Issues: \`$total_nuclei_js\`\n"
        discord_message+="‚Ä¢ Nuclei Alive Issues: \`$total_nuclei_alive\`\n"
        discord_message+="‚Ä¢ Open Ports: \`$total_ports\`\n\n"
        discord_message+="**üìÅ Output:** \`$OUTPUT_DIR\`\n"
        discord_message+="**‚è∞ Completed:** $(date '+%Y-%m-%d %H:%M:%S')"
        
        local payload=$(cat <<EOF
{
    "content": "$discord_message",
    "username": "Rezon Bot",
    "avatar_url": "https://cdn.discordapp.com/emojis/1234567890.png"
}
EOF
        )
        
        local response=$(curl -s -w "%{http_code}" -X POST -H "Content-Type: application/json" -d "$payload" "$WEBHOOK_URL")
        local http_code="${response: -3}"
        
        if [ "$http_code" = "200" ] || [ "$http_code" = "204" ]; then
            log_success "Detailed results webhook sent successfully"
        else
            log_warning "Failed to send results webhook (HTTP: $http_code)"
        fi
    fi
}

# Send error webhook notification
send_error_webhook() {
    local error_message="$1"
    local error_details="$2"
    
    if [[ "$WEBHOOK_ENABLED" == "true" && -n "$WEBHOOK_URL" ]]; then
        local discord_message="**‚ùå Rezon Reconnaissance Error**\n\n"
        discord_message+="**Error:** $error_message\n"
        discord_message+="**Details:** $error_details\n"
        discord_message+="**Timestamp:** $(date '+%Y-%m-%d %H:%M:%S')\n"
        discord_message+="**Output Directory:** \`$OUTPUT_DIR\`"
        
        local payload=$(cat <<EOF
{
    "content": "$discord_message",
    "username": "Rezon Bot",
    "avatar_url": "https://cdn.discordapp.com/emojis/1234567890.png"
}
EOF
        )
        
        local response=$(curl -s -w "%{http_code}" -X POST -H "Content-Type: application/json" -d "$payload" "$WEBHOOK_URL")
        local http_code="${response: -3}"
        
        if [ "$http_code" = "200" ] || [ "$http_code" = "204" ]; then
            log_success "Error webhook notification sent successfully"
        else
            log_warning "Failed to send error webhook (HTTP: $http_code)"
        fi
    fi
}

# Test webhook functionality
test_webhook() {
    if [[ "$WEBHOOK_ENABLED" == "true" && -n "$WEBHOOK_URL" ]]; then
        log "Testing webhook functionality..."
        send_webhook_notification "test" "Webhook test" "This is a test message to verify webhook functionality"
        log_success "Webhook test completed"
    else
        log_warning "Webhook not enabled or URL not configured"
        log "To enable webhook, set WEBHOOK_ENABLED=true and WEBHOOK_URL in config file"
    fi
}

# =============================================================================
# Main Processing Functions
# =============================================================================

# Process a single domain
process_domain() {
    local domain="$1"
    log "Processing domain: $domain"
    
    discover_subdomains "$domain"
    
    log_success "Completed subdomain discovery for $domain"
}

# Process all discovered subdomains with httpx-toolkit, katana, and gau
process_all_subdomains() {
    log "Processing all discovered subdomains with comprehensive tools..."
    
    # First, combine all subdomains into a single file
    log "Combining all discovered subdomains..."
    cat "$OUTPUT_DIR"/subdomains/*_subdomains.txt 2>/dev/null | sort -u > "$OUTPUT_DIR/all_subdomains.txt"
    
    if [ ! -s "$OUTPUT_DIR/all_subdomains.txt" ]; then
        log_warning "No subdomains found to process"
        return 0
    fi
    
    local subdomain_count=$(wc -l < "$OUTPUT_DIR/all_subdomains.txt")
    log_success "Combined $subdomain_count unique subdomains"
    
    # Send subdomain discovery webhook
    send_webhook_notification "progress" "Subdomain discovery completed" "Found $subdomain_count unique subdomains"
    
    # Run httpx-toolkit for alive domain detection and port scanning
    log "Running httpx-toolkit for alive domain detection and port scanning..."
    run_httpx
    
    # Run nuclei tech detection on alive domains
    log "Running nuclei tech detection on alive domains..."
    run_nuclei_tech
    
    # Run nmap against alive domains for comprehensive port scanning
    log "Running nmap against alive domains for comprehensive port scanning..."
    run_nmap
    
    # Extract URLs from all subdomains
    log "Extracting URLs from all discovered subdomains..."
    extract_urls
    
    # Filter JavaScript URLs
    log "Filtering JavaScript URLs..."
    filter_js_urls
    
    # Filter sensitive URLs
    log "Filtering sensitive URLs..."
    filter_sensitive_urls
    
    # Filter XSS URLs using gf
    log "Filtering XSS URLs using gf patterns..."
    filter_xss_urls
    
    # Filter SQL injection URLs using gf
    log "Filtering SQL injection URLs using gf patterns..."
    filter_sqli_urls
    
    # Run jshunter on JavaScript URLs
    log "Running jshunter on JavaScript URLs..."
    run_jshunter
    
    # Run nuclei on JavaScript URLs with tokens templates
    log "Running nuclei on JavaScript URLs with tokens templates..."
    run_nuclei_js
    
    # Run nuclei on alive domains with rezon templates
    log "Running nuclei on alive domains with rezon templates..."
    run_nuclei_alive
    
    log_success "Completed processing all subdomains"
}

# Process multiple domains from file using file-based approach
process_domains_file() {
    local domains_file="$1"
    log "Processing domains from file: $domains_file"
    
    if [ ! -f "$domains_file" ]; then
        log_error "Domains file not found: $domains_file"
        exit 1
    fi
    
    # Count total domains
    local total_domains=$(grep -v '^[[:space:]]*#' "$domains_file" | grep -v '^[[:space:]]*$' | wc -l)
    
    echo ""
    log "Starting file-based processing of $total_domains domains..."
    echo ""
    
    # Run subdomain discovery tools with file input
    run_subdomain_discovery_file "$domains_file"
}

# Run subdomain discovery tools with file input (file-based approach)
run_subdomain_discovery_file() {
    local domains_file="$1"
    
    log "Starting file-based subdomain discovery..."
    
    # Run subfinder with domain list file
    run_subfinder_file "$domains_file"
    
    # Run findomain with domain list file
    run_findomain_file "$domains_file"
    
    
    # Run assetfinder with stdin approach
    run_assetfinder_file "$domains_file"
    
    # Run subdominator with domain list file
    run_subdominator_file "$domains_file"
    
    # Combine all subdomain results
    combine_subdomain_results_file "$domains_file"
}

# Combine all subdomain results from file-based processing
combine_subdomain_results_file() {
    local domains_file="$1"
    local combined_output="$OUTPUT_DIR/all_subdomains.txt"
    
    log "Combining all subdomain results from file-based processing..."
    
    # Create combined output file
    > "$combined_output"
    
    # Combine results from all tools
    local tool_files=(
        "$OUTPUT_DIR/subdomains/all_subfinder_subdomains.txt"
        "$OUTPUT_DIR/subdomains/all_findomain_subdomains.txt"
        "$OUTPUT_DIR/subdomains/all_assetfinder_subdomains.txt"
        "$OUTPUT_DIR/subdomains/all_subdominator_subdomains.txt"
    )
    
    for tool_file in "${tool_files[@]}"; do
        if [ -f "$tool_file" ] && [ -s "$tool_file" ]; then
            cat "$tool_file" >> "$combined_output"
        fi
    done
    
    # Remove duplicates and sort
    if [ -s "$combined_output" ]; then
        sort -u "$combined_output" -o "$combined_output"
        local count=$(wc -l < "$combined_output")
        log_success "Combined subdomain results: $count unique subdomains from all tools"
    else
        log_warning "No subdomains found from any tool"
    fi
}

# Process domains in parallel
process_domains_parallel() {
    local domains_file="$1"
    local total_domains="$2"
    local domain_count=0
    local pids=()
    local completed=0
    
    # Create a temporary file for domain list
    local temp_domains="/tmp/domains_$$.txt"
    grep -v '^[[:space:]]*#' "$domains_file" | grep -v '^[[:space:]]*$' | xargs -I {} echo {} > "$temp_domains"
    
    while IFS= read -r domain || [ -n "$domain" ]; do
        # Remove leading/trailing whitespace
        domain=$(echo "$domain" | xargs)
        
        if [[ -n "$domain" ]]; then
            # Start processing domain in background
            process_domain "$domain" &
            local pid=$!
            pids+=($pid)
            ((domain_count++))
            
            # Limit parallel jobs
            if [[ ${#pids[@]} -ge $MAX_PARALLEL_JOBS ]]; then
                # Wait for one job to complete
                wait ${pids[0]}
                pids=("${pids[@]:1}")
                ((completed++))
                show_progress $completed $total_domains "Processing domains"
            fi
        fi
    done < "$temp_domains"
    
    # Wait for remaining jobs
    for pid in "${pids[@]}"; do
        wait $pid
        ((completed++))
        show_progress $completed $total_domains "Processing domains"
    done
    
    # Cleanup
    rm -f "$temp_domains"
    
    echo ""
    log_success "Processed $domain_count domains from file"
}

# =============================================================================
# Main Script Logic
# =============================================================================

# Display banner
display_banner() {
    echo -e "${BLUE}"
    echo "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà"
    echo "‚ñë‚ñë‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñë‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñë‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñë‚ñë‚ñà‚ñà‚ñà "
    echo " ‚ñë‚ñà‚ñà‚ñà    ‚ñë‚ñà‚ñà‚ñà  ‚ñë‚ñà‚ñà‚ñà  ‚ñà ‚ñë ‚ñë     ‚ñà‚ñà‚ñà‚ñë   ‚ñà‚ñà‚ñà     ‚ñë‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà "
    echo " ‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        ‚ñà‚ñà‚ñà    ‚ñë‚ñà‚ñà‚ñà      ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà "
    echo " ‚ñë‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà  ‚ñë‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà       ‚ñà‚ñà‚ñà     ‚ñë‚ñà‚ñà‚ñà      ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà ‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà "
    echo " ‚ñë‚ñà‚ñà‚ñà    ‚ñë‚ñà‚ñà‚ñà  ‚ñë‚ñà‚ñà‚ñà ‚ñë   ‚ñà  ‚ñà‚ñà‚ñà‚ñà     ‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà     ‚ñà‚ñà‚ñà  ‚ñë‚ñà‚ñà‚ñà  ‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà "
    echo " ‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë   ‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà"
    echo "‚ñë‚ñë‚ñë‚ñë‚ñë   ‚ñë‚ñë‚ñë‚ñë‚ñë ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë    ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë    ‚ñë‚ñë‚ñë‚ñë‚ñë    ‚ñë‚ñë‚ñë‚ñë‚ñë "
    echo ""
    echo "                    Advanced Reconnaissance Script"
    echo "                         for Bug Hunting"
    echo "============================================================================="
    echo -e "${NC}"
    echo "This script will:"
    echo "  ‚Ä¢ Discover subdomains using multiple tools"
    echo "  ‚Ä¢ Extract URLs using Katana and Gau"
    echo "  ‚Ä¢ Filter JavaScript URLs"
    echo "  ‚Ä¢ Identify sensitive file URLs"
    echo "  ‚Ä¢ Generate comprehensive reports"
    echo ""
}

# Display usage information
usage() {
    echo "Usage: $0 [OPTIONS] <domain_or_file>"
    echo ""
    echo "Options:"
    echo "  -d, --domain   Process a single domain"
    echo "  -f, --file     Process domains from a file"
    echo "  -t, --test     Test webhook functionality"
    echo "  -h, --help     Show this help message"
    echo "  -v, --verbose  Enable verbose output"
    echo ""
    echo "Examples:"
    echo "  $0 example.com"
    echo "  $0 -d example.com"
    echo "  $0 --domain example.com"
    echo "  $0 -f domains.txt"
    echo "  $0 --file /path/to/domains.txt"
    echo ""
    echo "Required tools:"
    printf "  %s\n" "${REQUIRED_TOOLS[@]}"
}

# Main function
main() {
    local domains_file=""
    local verbose=false
    
    # Parse command line arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -d|--domain)
                if [[ -n "$2" ]]; then
                    if ! validate_domain "$2"; then
                        log_error "Invalid domain format: $2"
                        exit 1
                    fi
                    # Display banner and initialize the script properly
                    display_banner
                    load_config
                    check_tools
                    setup_directories
                    process_domain "$2"
                    process_all_subdomains
                    create_combined_outputs
                    if [[ "$GENERATE_STATISTICS" == "true" ]]; then
                        generate_statistics
                    fi
                    exit 0
                else
                    log_error "No domain specified after -d/--domain"
                    usage
                    exit 1
                fi
                ;;
            -f|--file)
                domains_file="$2"
                shift 2
                ;;
            -h|--help)
                usage
                exit 0
                ;;
            -t|--test)
                display_banner
                load_config
                test_webhook
                exit 0
                ;;
            -v|--verbose)
                verbose=true
                shift
                ;;
            -*)
                log_error "Unknown option: $1"
                usage
                exit 1
                ;;
            *)
                if [[ -z "$domains_file" && -z "$1" ]]; then
                    log_error "No domain or file specified"
                    usage
                    exit 1
                fi
                break
                ;;
        esac
    done
    
    # Display banner
    display_banner
    
    # Load configuration
    load_config
    
    # Check for required tools
    check_tools
    
    # Setup output directories
    setup_directories
    
    # Send start webhook notification
    send_webhook_notification "started" "Reconnaissance started" "Initializing reconnaissance process"
    
    # Set up cleanup trap
    trap cleanup EXIT
    
    # Process input
    if [[ -n "$domains_file" ]]; then
        if ! validate_file "$domains_file"; then
            log_error "Domains file not found or not readable: $domains_file"
            exit 1
        fi
        
        # Clean and validate domain list
        local cleaned_file="$OUTPUT_DIR/cleaned_domains.txt"
        if ! clean_domain_list "$domains_file" "$cleaned_file"; then
            log_error "No valid domains found in file"
            exit 1
        fi
        
        process_domains_file "$cleaned_file"
    else
        if [[ -z "$1" ]]; then
            log_error "No domain specified"
            usage
            exit 1
        fi
        
        if ! validate_domain "$1"; then
            log_error "Invalid domain format: $1"
            exit 1
        fi
        
        process_domain "$1"
    fi
    
    # Process all discovered subdomains with comprehensive tools
    process_all_subdomains
    
    # Create combined outputs
    create_combined_outputs
    
    # Generate statistics
    if [[ "$GENERATE_STATISTICS" == "true" ]]; then
        generate_statistics
    fi
    
    # Send detailed completion webhook
    send_results_webhook
    
    # Display summary
    echo ""
    log_success "Reconnaissance completed successfully!"
    echo ""
    echo "Results saved in: $OUTPUT_DIR"
    echo "  ‚Ä¢ Subdomains: $OUTPUT_DIR/all_subdomains.txt"
    echo "  ‚Ä¢ All URLs: $OUTPUT_DIR/all_urls.txt"
    echo "  ‚Ä¢ JavaScript URLs: $OUTPUT_DIR/all_js_urls.txt"
    echo "  ‚Ä¢ Sensitive URLs: $OUTPUT_DIR/sensitiveurls.txt"
    echo "  ‚Ä¢ Filtered URLs: $OUTPUT_DIR/filtered_urls.txt"
    echo "  ‚Ä¢ Statistics: $OUTPUT_DIR/statistics.txt"
    echo "  ‚Ä¢ Log file: $LOG_FILE"
    echo ""
}

# Run main function with all arguments
main "$@"
